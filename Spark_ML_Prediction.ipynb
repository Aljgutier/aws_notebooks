{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark_ML_Prediction\n",
    "\n",
    "* CA Housing Data Set\n",
    "* Spark EMR Notebok on Spark 2.4.4\n",
    "* Recreate examples from Learning Spark 2nd Edition, Chapter 10 \n",
    "  * https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/\n",
    "  * Spark version 3.0 ... ML examples were converted to Spark 2.4.4\n",
    "  * Use dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5619061c414d4a26aa43cd966fdc941a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1607971315187_0003</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-43-43.us-west-2.compute.internal:20888/proxy/application_1607971315187_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-43-43.us-west-2.compute.internal:8042/node/containerlogs/container_1607971315187_0003_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.types.{IntegerType, DoubleType, FloatType, StringType, StructField, StructType}\n",
      "import org.apache.spark.SparkConf\n",
      "import org.apache.spark.sql.{DataFrame, SparkSession}\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{IntegerType, DoubleType, FloatType, StringType, StructField, StructType}\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a56964fea5b493aa8a92ec5a0dce5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path_train: String = s3://data-ca-housing/Train\n",
      "path_test: String = s3://data-ca-housing/Test\n",
      "customSchema: org.apache.spark.sql.types.StructType = StructType(StructField(longitude,DoubleType,true), StructField(latitude,DoubleType,true), StructField(housing_median_age,DoubleType,true), StructField(total_rooms,DoubleType,true), StructField(total_bedrooms,DoubleType,true), StructField(population,DoubleType,true), StructField(households,DoubleType,true), StructField(median_income,DoubleType,true), StructField(median_house_value,DoubleType,true), StructField(ocean_proximity,StringType,true))\n",
      "trainDF: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 8 more fields]\n",
      "testDF: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 8 more fields]\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|  -124.26|   40.58|              52.0|     2217.0|         394.0|     907.0|     369.0|       2.3571|          111400.0|     NEAR OCEAN|\n",
      "|  -124.23|   40.54|              52.0|     2694.0|         453.0|    1152.0|     435.0|       3.0806|          106700.0|     NEAR OCEAN|\n",
      "|  -124.23|   40.81|              52.0|     1112.0|         209.0|     544.0|     172.0|       3.3462|           50800.0|     NEAR OCEAN|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|  -124.35|   40.54|              52.0|     1820.0|         300.0|     806.0|     270.0|       3.0147|           94600.0|     NEAR OCEAN|\n",
      "|   -124.3|   41.84|              17.0|     2677.0|         531.0|    1244.0|     456.0|       3.0313|          103600.0|     NEAR OCEAN|\n",
      "|  -124.27|   40.69|              36.0|     2349.0|         528.0|    1194.0|     465.0|       2.5179|           79000.0|     NEAR OCEAN|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val path_train=\"s3://data-ca-housing/Train\"\n",
    "val path_test=\"s3://data-ca-housing/Test\"\n",
    "\n",
    "val customSchema = StructType(Array(\n",
    "    StructField(\"longitude\", DoubleType, true),\n",
    "    StructField(\"latitude\", DoubleType, true),\n",
    "    StructField(\"housing_median_age\", DoubleType, true),\n",
    "    StructField(\"total_rooms\", DoubleType, true),\n",
    "    StructField(\"total_bedrooms\", DoubleType, true),\n",
    "    StructField(\"population\", DoubleType, true),\n",
    "    StructField(\"households\", DoubleType, true),\n",
    "    StructField(\"median_income\", DoubleType, true),\n",
    "    StructField(\"median_house_value\", DoubleType, true),\n",
    "    StructField(\"ocean_proximity\", StringType, true )));\n",
    "\n",
    "val trainDF = spark.read.format(\"com.databricks.spark.csv\").\n",
    "  option(\"delimiter\", \",\").schema(customSchema).load(path_train)\n",
    "\n",
    "val testDF = spark.read.format(\"com.databricks.spark.csv\").\n",
    "  option(\"delimiter\", \",\").schema(customSchema).load(path_test)\n",
    "\n",
    "trainDF.show(3)\n",
    "testDF.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression One Variable**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features should go through a scaler ... the example from book did not do so, should be able to add it fairly simply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48dbfc249dcb48cabf400557ab91b814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.feature.VectorAssembler\n",
      "vecAssembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_e0f967e91029\n",
      "vecTrainDF: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 9 more fields]\n",
      "+-------------+--------+------------------+\n",
      "|median_income|features|median_house_value|\n",
      "+-------------+--------+------------------+\n",
      "|       2.3571|[2.3571]|          111400.0|\n",
      "|       3.0806|[3.0806]|          106700.0|\n",
      "|       3.3462|[3.3462]|           50800.0|\n",
      "|       2.4805|[2.4805]|           73200.0|\n",
      "|       2.0074|[2.0074]|           66900.0|\n",
      "|       2.5795|[2.5795]|           68400.0|\n",
      "|       3.5363|[3.5363]|           90100.0|\n",
      "|        2.442| [2.442]|           69000.0|\n",
      "|       3.0536|[3.0536]|          107000.0|\n",
      "|       2.5096|[2.5096]|           72200.0|\n",
      "+-------------+--------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "import org.apache.spark.ml.regression.LinearRegression\n",
      "lr: org.apache.spark.ml.regression.LinearRegression = linReg_f2a807761c52\n",
      "lrModel: org.apache.spark.ml.regression.LinearRegressionModel = linReg_f2a807761c52\n",
      "m: Double = 42861.62343831498\n",
      "b: Double = 40063.45619847035\n",
      "\n",
      "The formula for the linear regression line is\n",
      "  price = 42861.62 x median_income + 40063.46\n",
      "import org.apache.spark.ml.Pipeline\n",
      "pipeline: org.apache.spark.ml.Pipeline = pipeline_c07d8c41dc90\n",
      "pipelineModel: org.apache.spark.ml.PipelineModel = pipeline_c07d8c41dc90\n",
      "predDF: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 10 more fields]\n",
      "+-------------+--------+------------------+------------------+\n",
      "|median_income|features|median_house_value|        prediction|\n",
      "+-------------+--------+------------------+------------------+\n",
      "|       3.0147|[3.0147]|           94600.0|169278.39237795852|\n",
      "|       3.0313|[3.0313]|          103600.0|169989.89532703452|\n",
      "|       2.5179|[2.5179]|           79000.0| 147984.7378538036|\n",
      "|       2.1607|[2.1607]|           67000.0|132674.56596163753|\n",
      "|       2.2566|[2.2566]|           92700.0|136784.99564937194|\n",
      "|       3.1406|[3.1406]|           96000.0|174674.67076884236|\n",
      "|       2.0938|[2.0938]|           79600.0|129807.12335361424|\n",
      "|       3.1711|[3.1711]|          104200.0|  175981.950283711|\n",
      "|       2.9143|[2.9143]|           99600.0|164975.08538475167|\n",
      "|          2.5|   [2.5]|           72600.0| 147217.5147942578|\n",
      "+-------------+--------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Vector assembler takes a list of input columns and creates DF with features\n",
    "import org.apache.spark.ml.feature.VectorAssembler \n",
    "val vecAssembler = new VectorAssembler (). \n",
    "    setInputCols(Array(\"median_income\" )). \n",
    "    setOutputCol ( \"features\" ) \n",
    "\n",
    "val vecTrainDF = vecAssembler.transform(trainDF) \n",
    "vecTrainDF.select(\"median_income\" , \"features\" , \"median_house_value\" ).show (10)\n",
    "\n",
    "// Linear Regression Model\n",
    "import org.apache.spark.ml.regression.LinearRegression \n",
    "val lr = new LinearRegression ().setFeaturesCol(\"features\"). \n",
    "    setLabelCol ( \"median_house_value\") \n",
    "\n",
    "// Fit\n",
    "val lrModel = lr.fit (vecTrainDF) \n",
    "val m = lrModel.coefficients(0) \n",
    "val b = lrModel.intercept \n",
    "\n",
    "println(f\"\"\"\n",
    "The formula for the linear regression line is\n",
    "  price = $m%1.2f x median_income + $b%1.2f\"\"\") \n",
    "\n",
    "// Create Pipeline\n",
    "import org.apache.spark.ml.Pipeline \n",
    "val pipeline = new Pipeline().setStages(Array(vecAssembler,lr)) \n",
    "val pipelineModel = pipeline.fit(trainDF) \n",
    "\n",
    "// Apply Pipelne to Test Data\n",
    "val predDF = pipelineModel.transform(testDF) \n",
    "predDF.select( \"median_income\" , \"features\" , \"median_house_value\" , \"prediction\" ).show (10) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression:  Numerical + Categorical Features**\n",
    "\n",
    "> FEATURES SHOULD BE \"STANDARDIZED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75dc6a81008249c996aa5a3062cc6f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.feature.StringIndexer\n",
      "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
      "categoricalCols: String = ocean_proximity\n",
      "indexOutputCols: String = ocean_proximity_index\n",
      "oheOutputCols: String = ocean_proximity_ohe\n",
      "categoricalCols = ocean_proximity\n",
      "indexOuputCols = ocean_proximity_index\n",
      "oheOutputCols = ocean_proximity_ohe\n",
      "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_0da34fa48eef\n",
      "oheEstimator: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEncoder_686d1d0ab551\n",
      "import org.apache.spark.ml.feature.Imputer\n",
      "imputer: org.apache.spark.ml.feature.Imputer = imputer_0f5abefa67ff\n",
      "numericalCols: Array[String] = Array(longitude, latitude, housing_median_age, total_rooms, total_bedrooms_imp, population, households, median_income)\n",
      "import org.apache.spark.ml.feature.VectorAssembler\n",
      "assemblerInputs: Array[String] = Array(longitude, latitude, housing_median_age, total_rooms, total_bedrooms_imp, population, households, median_income, ocean_proximity_ohe)\n",
      "vecAssembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_ec9e897f2d15\n",
      "import org.apache.spark.ml.regression.LinearRegression\n",
      "lr: org.apache.spark.ml.regression.LinearRegression = linReg_4850f71a12ab\n",
      "import org.apache.spark.ml.Pipeline\n",
      "pipeline: org.apache.spark.ml.Pipeline = pipeline_ba4fc4fbd499\n",
      "pipelineModel: org.apache.spark.ml.PipelineModel = pipeline_ba4fc4fbd499\n",
      "predDF: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 13 more fields]\n",
      "+-------------+--------------------+------------------+------------------+\n",
      "|median_income|            features|median_house_value|        prediction|\n",
      "+-------------+--------------------+------------------+------------------+\n",
      "|       3.0147|[-124.35,40.54,52...|           94600.0| 220085.8854208463|\n",
      "|       3.0313|[-124.3,41.84,17....|          103600.0|147713.26855634106|\n",
      "|       2.5179|[-124.27,40.69,36...|           79000.0|188515.86338644987|\n",
      "|       2.1607|[-124.18,40.78,34...|           67000.0|155297.99711679993|\n",
      "|       2.2566|[-124.16,40.79,52...|           92700.0| 192359.1135302619|\n",
      "|       3.1406|[-124.16,40.95,20...|           96000.0|167649.06440826925|\n",
      "|       2.0938|[-124.15,40.59,39...|           79600.0|164232.51637098799|\n",
      "|       3.1711|[-124.15,40.78,41...|          104200.0|203715.95818037773|\n",
      "|       2.9143|[-124.14,40.77,27...|           99600.0|186731.01577399904|\n",
      "|          2.5|[-124.14,40.8,32....|           72600.0| 163250.9499483942|\n",
      "+-------------+--------------------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
      "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_eac42e9dc615\n",
      "rmse: Double = 69904.32636312087\n",
      "RMSE = 69904.32636312087\n"
     ]
    }
   ],
   "source": [
    "// Numerial and Categorical Features\n",
    "\n",
    "import org.apache.spark.ml.feature.StringIndexer \n",
    "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
    "\n",
    "val categoricalCols = \"ocean_proximity\"\n",
    "val indexOutputCols = \"ocean_proximity_index\"\n",
    "val oheOutputCols   = \"ocean_proximity_ohe\"\n",
    "//val assemblerInputs = Array(oheOutputCols) ++ numericalCols\n",
    "\n",
    "println(\"categoricalCols = \" + categoricalCols)\n",
    "println(\"indexOuputCols = \" + indexOutputCols)\n",
    "println(\"oheOutputCols = \" + oheOutputCols)\n",
    "\n",
    "\n",
    "// below works for array of categorical names, but not for just 1 so set manually below\n",
    "//val stringIndexer = new StringIndexer(inputCols=categoricalCols, outputCols = indexOutputCols)\n",
    "// \n",
    "\n",
    "// Spark 2.4.4 Indexer Output requires String input ... below OHE requires Array\n",
    "//  indexer wants String input given this is for one column so must keep cols as strings \n",
    "//  then apply Array when needed\n",
    "val indexer = new StringIndexer().\n",
    "    setInputCol(categoricalCols).\n",
    "    setOutputCol(indexOutputCols).\n",
    "    setHandleInvalid(\"skip\")\n",
    "\n",
    "// OHE Encoder Estimator Spark vs 2.4.4 ... Transform\n",
    "val oheEstimator= new OneHotEncoderEstimator().\n",
    "    setInputCols(Array(indexOutputCols)).\n",
    "    setOutputCols(Array(oheOutputCols))\n",
    "\n",
    "\n",
    "// Some features will have nulls so need to do some imputation\n",
    "// total bedrooms has nulls ... set imputation column to median\n",
    "//  note, other cols are clean, but if Lat/Long had null may want to dropna\n",
    "import org.apache.spark.ml.feature.Imputer\n",
    "val imputer = new Imputer().\n",
    "    setInputCols(Array(\"total_bedrooms\")).\n",
    "    setOutputCols(Array(\"total_bedrooms_imp\"))\n",
    "\n",
    "//create features\n",
    "val numericalCols=Array(\"longitude\",\"latitude\",\"housing_median_age\",\"total_rooms\", \"total_bedrooms_imp\",\"population\",\"households\",\"median_income\")\n",
    "\n",
    "import org.apache.spark.ml.feature.VectorAssembler \n",
    "val assemblerInputs=numericalCols++Array(oheOutputCols)\n",
    "val vecAssembler = new VectorAssembler(). \n",
    "    setInputCols(assemblerInputs). \n",
    "    setOutputCol ( \"features\" )\n",
    "\n",
    "\n",
    "// Linear Regression Model\n",
    "import org.apache.spark.ml.regression.LinearRegression \n",
    "val lr = new LinearRegression (). \n",
    "    setLabelCol ( \"median_house_value\" ). \n",
    "    setFeaturesCol ( \"features\" ) \n",
    "\n",
    "import org.apache.spark.ml.Pipeline \n",
    "val pipeline = new Pipeline(). \n",
    "    setStages (Array(indexer, oheEstimator, imputer, vecAssembler, lr))\n",
    "\n",
    "// Fit pipeline\n",
    "val pipelineModel = pipeline.fit(trainDF) \n",
    "\n",
    "// Apply Pipelne to Test Data\n",
    "val predDF = pipelineModel.transform(testDF) \n",
    "predDF.select( \"median_income\" , \"features\" , \"median_house_value\" , \"prediction\" ).show (10) \n",
    "\n",
    "\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "val evaluator = new RegressionEvaluator().\n",
    "  setMetricName(\"rmse\").\n",
    "  setLabelCol(\"median_house_value\").\n",
    "  setPredictionCol(\"prediction\")\n",
    "\n",
    "val rmse = evaluator.evaluate(predDF)\n",
    "println(s\"RMSE = $rmse\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are relatively fast to build, **highly interpretable**, and **scale-invariant** (i.e., standardizing or scaling the numeric features does not change the performance of the tree). \n",
    "\n",
    "For decision trees, you don’t have to worry about standardizing or scaling your input features, because this has no impact on the splits but you do have to be careful about how you prepare your categorical features.\n",
    "\n",
    "Tree-based methods can naturally handle categorical variables. In spark.ml , you just need to pass the categorical columns to the StringIndexer , and the decision tree can take care of the rest. \n",
    "\n",
    "**Potential Error**\n",
    "\n",
    "This produces the following error: java . lang . IllegalArgumentException : requirement failed: DecisionTree requires maxBins (= 32 ) to be at least as large as the number of values in each categorical feature , but categorical feature 3 has 36 values . Consider removing this and other categorical features with a large number of values , or add more training examples \n",
    "\n",
    "This produces the following error: java . lang . IllegalArgumentException : requirement failed: DecisionTree requires maxBins (= 32 ) to be at least as large as the number of values in each categorical feature , but categorical feature 3 has 36 values . Consider removing this and other categorical features with a large number of values , or add more training examples \n",
    "\n",
    "This produces the following error: java . lang . IllegalArgumentException : requirement failed: DecisionTree requires maxBins (= 32 ) to be at least as large as the number of values in each categorical feature , but categorical feature 3 has 36 values . Consider removing this and other categorical features with a large number of values , or add more training examples \n",
    "\n",
    "// In Scala \n",
    "dt.setMaxBins(40) \n",
    "val pipelineModel = pipeline.fit(trainDF) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "386108b97bd945839293885e6479199a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.feature.StringIndexer\n",
      "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
      "categoricalCols: String = ocean_proximity\n",
      "indexOutputCols: String = ocean_proximity_index\n",
      "oheOutputCols: String = ocean_proximity_ohe\n",
      "categoricalCols = ocean_proximity\n",
      "indexOuputCols = ocean_proximity_index\n",
      "oheOutputCols = ocean_proximity_ohe\n",
      "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_54cf0987c446\n",
      "import org.apache.spark.ml.feature.Imputer\n",
      "imputer: org.apache.spark.ml.feature.Imputer = imputer_6ae604ec1656\n",
      "numericalCols: Array[String] = Array(longitude, latitude, housing_median_age, total_rooms, total_bedrooms_imp, population, households, median_income)\n",
      "import org.apache.spark.ml.feature.VectorAssembler\n",
      "assemblerInputs: Array[String] = Array(longitude, latitude, housing_median_age, total_rooms, total_bedrooms_imp, population, households, median_income, ocean_proximity_index)\n",
      "vecAssembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_5ce6371fda63\n",
      "import org.apache.spark.ml.regression.DecisionTreeRegressor\n",
      "dt: org.apache.spark.ml.regression.DecisionTreeRegressor = dtr_097f5fc533e4\n",
      "import org.apache.spark.ml.Pipeline\n",
      "pipeline: org.apache.spark.ml.Pipeline = pipeline_c38f53c0f3b5\n",
      "pipelineModel: org.apache.spark.ml.PipelineModel = pipeline_c38f53c0f3b5\n",
      "predDF: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 12 more fields]\n",
      "+-------------+--------------------+------------------+------------------+\n",
      "|median_income|            features|median_house_value|        prediction|\n",
      "+-------------+--------------------+------------------+------------------+\n",
      "|       3.0147|[-124.35,40.54,52...|           94600.0|154633.33333333334|\n",
      "|       3.0313|[-124.3,41.84,17....|          103600.0|154633.33333333334|\n",
      "|       2.5179|[-124.27,40.69,36...|           79000.0|134017.33238636365|\n",
      "|       2.1607|[-124.18,40.78,34...|           67000.0|134017.33238636365|\n",
      "|       2.2566|[-124.16,40.79,52...|           92700.0|134017.33238636365|\n",
      "|       3.1406|[-124.16,40.95,20...|           96000.0|154633.33333333334|\n",
      "|       2.0938|[-124.15,40.59,39...|           79600.0|134017.33238636365|\n",
      "|       3.1711|[-124.15,40.78,41...|          104200.0|154633.33333333334|\n",
      "|       2.9143|[-124.14,40.77,27...|           99600.0|154633.33333333334|\n",
      "|          2.5|[-124.14,40.8,32....|           72600.0|134017.33238636365|\n",
      "+-------------+--------------------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
      "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_4fe90662638d\n",
      "rmse: Double = 69549.52546933212\n",
      "RMSE = 69549.52546933212\n",
      "dtModel: org.apache.spark.ml.regression.DecisionTreeRegressionModel = DecisionTreeRegressionModel (uid=dtr_097f5fc533e4) of depth 5 with 63 nodes\n",
      "featureImp: Array[(String, Double)] = Array((longitude,0.10589399121135375), (latitude,0.10925904543587145), (housing_median_age,0.03823966819180515), (total_rooms,0.0), (total_bedrooms_imp,0.0), (population,0.0), (households,0.0), (median_income,0.2997772870118915), (ocean_proximity_index,0.44683000814907814))\n",
      "columns: Array[String] = Array(feature, Importance)\n",
      "featureImpDF: org.apache.spark.sql.DataFrame = [feature: string, Importance: double]\n",
      "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
      "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_90a8902d1d71\n",
      "rmse: Double = 69549.52546933212\n",
      "RMSE = 69549.52546933212\n",
      "+--------------------+-------------------+\n",
      "|             feature|         Importance|\n",
      "+--------------------+-------------------+\n",
      "|ocean_proximity_i...|0.44683000814907814|\n",
      "|       median_income| 0.2997772870118915|\n",
      "|            latitude|0.10925904543587145|\n",
      "|           longitude|0.10589399121135375|\n",
      "|  housing_median_age|0.03823966819180515|\n",
      "|         total_rooms|                0.0|\n",
      "|  total_bedrooms_imp|                0.0|\n",
      "|          population|                0.0|\n",
      "|          households|                0.0|\n",
      "+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Numerial and Categorical Features\n",
    "\n",
    "import org.apache.spark.ml.feature.StringIndexer \n",
    "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
    "\n",
    "val categoricalCols = \"ocean_proximity\"\n",
    "val indexOutputCols = \"ocean_proximity_index\"\n",
    "val oheOutputCols   = \"ocean_proximity_ohe\"\n",
    "//val assemblerInputs = Array(oheOutputCols) ++ numericalCols\n",
    "\n",
    "println(\"categoricalCols = \" + categoricalCols)\n",
    "println(\"indexOuputCols = \" + indexOutputCols)\n",
    "println(\"oheOutputCols = \" + oheOutputCols)\n",
    "\n",
    "\n",
    "// below works for array of categorical names, but not for just 1 so set manually below\n",
    "//val stringIndexer = new StringIndexer(inputCols=categoricalCols, outputCols = indexOutputCols)\n",
    "// \n",
    "\n",
    "// Spark 2.4.4 Indexer Output requires String input ... below OHE requires Array\n",
    "//  indexer wants String input given this is for one column so must keep cols as strings \n",
    "//  then apply Array when needed\n",
    "val indexer = new StringIndexer().\n",
    "    setInputCol(categoricalCols).\n",
    "    setOutputCol(indexOutputCols).\n",
    "    setHandleInvalid(\"skip\")\n",
    "\n",
    "\n",
    "// Some features will have nulls so need to do some imputation\n",
    "// total bedrooms has nulls ... set imputation column to median\n",
    "//  note, other cols are clean, but if Lat/Long had null may want to dropna\n",
    "import org.apache.spark.ml.feature.Imputer\n",
    "val imputer = new Imputer().\n",
    "    setInputCols(Array(\"total_bedrooms\")).\n",
    "    setOutputCols(Array(\"total_bedrooms_imp\"))\n",
    "\n",
    "//create features\n",
    "val numericalCols=Array(\"longitude\",\"latitude\",\"housing_median_age\",\"total_rooms\", \"total_bedrooms_imp\",\"population\",\"households\",\"median_income\")\n",
    "\n",
    "import org.apache.spark.ml.feature.VectorAssembler \n",
    "val assemblerInputs=numericalCols++Array(indexOutputCols)\n",
    "val vecAssembler = new VectorAssembler(). \n",
    "    setInputCols(assemblerInputs). \n",
    "    setOutputCol ( \"features\" )\n",
    "\n",
    "\n",
    "// Decision Tree Regression Model\n",
    "import org.apache.spark.ml.regression.DecisionTreeRegressor \n",
    "val dt = new DecisionTreeRegressor(). \n",
    "    setLabelCol ( \"median_house_value\" ). \n",
    "    //setMaxBins ( 40 ) \n",
    "    setFeaturesCol ( \"features\" ) \n",
    "\n",
    "import org.apache.spark.ml.Pipeline \n",
    "val pipeline = new Pipeline(). \n",
    "    setStages (Array(indexer, imputer, vecAssembler, dt))\n",
    "\n",
    "// Fit pipeline\n",
    "val pipelineModel = pipeline.fit(trainDF) \n",
    "\n",
    "// Apply Pipelne to Test Data\n",
    "val predDF = pipelineModel.transform(testDF) \n",
    "predDF.select( \"median_income\" , \"features\" , \"median_house_value\" , \"prediction\" ).show (10) \n",
    "\n",
    "\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "val evaluator = new RegressionEvaluator().\n",
    "  setMetricName(\"rmse\").\n",
    "  setLabelCol(\"median_house_value\").\n",
    "  setPredictionCol(\"prediction\")\n",
    "\n",
    "val rmse = evaluator.evaluate(predDF)\n",
    "println(s\"RMSE = $rmse\")\n",
    "\n",
    "val dtModel = pipelineModel.stages.\n",
    "    last.asInstanceOf[org.apache.spark.ml.regression.DecisionTreeRegressionModel]\n",
    "\n",
    "val featureImp = vecAssembler.getInputCols.zip(dtModel.featureImportances.toArray) \n",
    "val columns = Array ( \"feature\" , \"Importance\" ) \n",
    "val featureImpDF = spark.createDataFrame (featureImp). \n",
    "    toDF (columns : _ * ) \n",
    "\n",
    "\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "val evaluator = new RegressionEvaluator().\n",
    "  setMetricName(\"rmse\").\n",
    "  setLabelCol(\"median_house_value\").\n",
    "  setPredictionCol(\"prediction\")\n",
    "\n",
    "val rmse = evaluator.evaluate(predDF)\n",
    "println(s\"RMSE = $rmse\")\n",
    "\n",
    "featureImpDF.orderBy($\"Importance\".desc ).show () "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Random Forest w/ HyperParameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d886c7ed9c45c5bf7d0fa38906bc04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.feature.StringIndexer\n",
      "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
      "categoricalCols: String = ocean_proximity\n",
      "indexOutputCols: String = ocean_proximity_index\n",
      "oheOutputCols: String = ocean_proximity_ohe\n",
      "categoricalCols = ocean_proximity\n",
      "indexOuputCols = ocean_proximity_index\n",
      "oheOutputCols = ocean_proximity_ohe\n",
      "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_859dfa7b36ae\n",
      "import org.apache.spark.ml.feature.Imputer\n",
      "imputer: org.apache.spark.ml.feature.Imputer = imputer_0f40dc2fe5c5\n",
      "numericalCols: Array[String] = Array(longitude, latitude, housing_median_age, total_rooms, total_bedrooms_imp, population, households, median_income)\n",
      "import org.apache.spark.ml.feature.VectorAssembler\n",
      "assemblerInputs: Array[String] = Array(longitude, latitude, housing_median_age, total_rooms, total_bedrooms_imp, population, households, median_income, ocean_proximity_index)\n",
      "vecAssembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_41ef137e85c9\n",
      "import org.apache.spark.ml.regression.RandomForestRegressor\n",
      "rf: org.apache.spark.ml.regression.RandomForestRegressor = rfr_485bf7d01124\n",
      "import org.apache.spark.ml.Pipeline\n",
      "pipeline: org.apache.spark.ml.Pipeline = pipeline_8924ab6795e1\n",
      "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
      "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
      "Array({\n",
      "\trfr_485bf7d01124-maxDepth: 2,\n",
      "\trfr_485bf7d01124-numTrees: 10\n",
      "}, {\n",
      "\trfr_485bf7d01124-maxDepth: 2,\n",
      "\trfr_485bf7d01124-numTrees: 100\n",
      "}, {\n",
      "\trfr_485bf7d01124-maxDepth: 4,\n",
      "\trfr_485bf7d01124-numTrees: 10\n",
      "}, {\n",
      "\trfr_485bf7d01124-maxDepth: 4,\n",
      "\trfr_485bf7d01124-numTrees: 100\n",
      "}, {\n",
      "\trfr_485bf7d01124-maxDepth: 6,\n",
      "\trfr_485bf7d01124-numTrees: 10\n",
      "}, {\n",
      "\trfr_485bf7d01124-maxDepth: 6,\n",
      "\trfr_485bf7d01124-numTrees: 100\n",
      "})\n",
      "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
      "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_9f6f9c718f0e\n",
      "import org.apache.spark.ml.tuning.CrossValidator\n",
      "cv: org.apache.spark.ml.tuning.CrossValidator = cv_f37356b1bc0b\n",
      "cvModel: org.apache.spark.ml.tuning.CrossValidatorModel = cv_f37356b1bc0b\n",
      "res327: Array[(org.apache.spark.ml.param.ParamMap, Double)] =\n",
      "Array(({\n",
      "\trfr_485bf7d01124-maxDepth: 2,\n",
      "\trfr_485bf7d01124-numTrees: 10\n",
      "},79968.97969793099), ({\n",
      "\trfr_485bf7d01124-maxDepth: 2,\n",
      "\trfr_485bf7d01124-numTrees: 100\n",
      "},80628.15596474006), ({\n",
      "\trfr_485bf7d01124-maxDepth: 4,\n",
      "\trfr_485bf7d01124-numTrees: 10\n",
      "},73247.4956844239), ({\n",
      "\trfr_485bf7d01124-maxDepth: 4,\n",
      "\trfr_485bf7d01124-numTrees: 100\n",
      "},72638.19429127341), ({\n",
      "\trfr_485bf7d01124-maxDepth: 6,\n",
      "\trfr_485bf7d01124-numTrees: 10\n",
      "},65996.00472668365), ({\n",
      "\trfr_485bf7d01124-maxDepth: 6,\n",
      "\trfr_485bf7d01124-numTrees: 100\n",
      "},65512.503317089395))\n"
     ]
    }
   ],
   "source": [
    "// Do this Later\n",
    "\n",
    "\n",
    "import org.apache.spark.ml.feature.StringIndexer \n",
    "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
    "\n",
    "val categoricalCols = \"ocean_proximity\"\n",
    "val indexOutputCols = \"ocean_proximity_index\"\n",
    "val oheOutputCols   = \"ocean_proximity_ohe\"\n",
    "//val assemblerInputs = Array(oheOutputCols) ++ numericalCols\n",
    "\n",
    "println(\"categoricalCols = \" + categoricalCols)\n",
    "println(\"indexOuputCols = \" + indexOutputCols)\n",
    "println(\"oheOutputCols = \" + oheOutputCols)\n",
    "\n",
    "\n",
    "// below works for array of categorical names, but not for just 1 so set manually below\n",
    "//val stringIndexer = new StringIndexer(inputCols=categoricalCols, outputCols = indexOutputCols)\n",
    "// \n",
    "\n",
    "// Spark 2.4.4 Indexer Output requires String input ... below OHE requires Array\n",
    "//  indexer wants String input given this is for one column so must keep cols as strings \n",
    "//  then apply Array when needed\n",
    "val indexer = new StringIndexer().\n",
    "    setInputCol(categoricalCols).\n",
    "    setOutputCol(indexOutputCols).\n",
    "    setHandleInvalid(\"skip\")\n",
    "\n",
    "\n",
    "// Some features will have nulls so need to do some imputation\n",
    "// total bedrooms has nulls ... set imputation column to median\n",
    "//  note, other cols are clean, but if Lat/Long had null may want to dropna\n",
    "import org.apache.spark.ml.feature.Imputer\n",
    "val imputer = new Imputer().\n",
    "    setInputCols(Array(\"total_bedrooms\")).\n",
    "    setOutputCols(Array(\"total_bedrooms_imp\"))\n",
    "\n",
    "//create features\n",
    "val numericalCols=Array(\"longitude\",\"latitude\",\"housing_median_age\",\"total_rooms\", \"total_bedrooms_imp\",\"population\",\"households\",\"median_income\")\n",
    "\n",
    "import org.apache.spark.ml.feature.VectorAssembler \n",
    "val assemblerInputs=numericalCols++Array(indexOutputCols)\n",
    "val vecAssembler = new VectorAssembler(). \n",
    "    setInputCols(assemblerInputs). \n",
    "    setOutputCol ( \"features\" )\n",
    "\n",
    "\n",
    "// Random Forest Regressor\n",
    "import org.apache.spark.ml.regression.RandomForestRegressor \n",
    "val rf = new RandomForestRegressor().\n",
    "    setLabelCol(\"median_house_value\"). \n",
    "    //setMaxBins (40). \n",
    "    setFeaturesCol ( \"features\" ) \n",
    "\n",
    "import org.apache.spark.ml.Pipeline \n",
    "val pipeline = new Pipeline(). \n",
    "    setStages (Array(indexer, imputer, vecAssembler, rf))\n",
    "\n",
    "// Hyperparameter search Grid\n",
    "import org.apache.spark.ml.tuning.ParamGridBuilder \n",
    "val paramGrid = new ParamGridBuilder(). \n",
    "    addGrid (rf.maxDepth, Array(2 , 4 , 6 )). \n",
    "    addGrid (rf.numTrees, Array(10, 100)). \n",
    "    build () \n",
    "\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "val evaluator = new RegressionEvaluator().\n",
    "  setMetricName(\"rmse\").\n",
    "  setLabelCol(\"median_house_value\").\n",
    "  setPredictionCol(\"prediction\")\n",
    "\n",
    "// Cross Validation\n",
    "import org.apache.spark.ml.tuning.CrossValidator \n",
    "val cv = new CrossValidator(). \n",
    "    setEstimator(pipeline). \n",
    "    setEvaluator (evaluator). \n",
    "    setEstimatorParamMaps(paramGrid). \n",
    "    setNumFolds (3). \n",
    "    setSeed (42) \n",
    "\n",
    "val cvModel = cv.fit (trainDF) \n",
    "\n",
    "cvModel.getEstimatorParamMaps.zip(cvModel.avgMetrics)\n",
    "\n",
    "// So, how many models did we just rain? If you answered 18 (6 = 3 x 2 )\n",
    "// Once you've identified the optimal hyperparameter configuration, how do you\n",
    "// combine those three (or k) models together? While some models might be easy\n",
    "// enough to average together, some are not. Therefore, Spark retrains your model\n",
    "// on the entire training data set once it has identified the optimal hyperparameter\n",
    "// configuration, so in the end we trained 19 models.\n",
    "// if you want to retrain teh intermediate models trained, you can set colectSubModes=True\n",
    "// in the CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
